{"cells":[{"cell_type":"markdown","source":["#**Wine Quality Analysis Exercise**\n","\n","We will now focus on our main objectives of building predictive models to predict the wine quality (low, medium and high) based on other features. We will be following the standard classification Machine Learning pipeline in this case. \n","\n","Adapted from Dipanjan Sarkar et al. 2018. [Practical Machine Learning with Python](https://link.springer.com/book/10.1007/978-1-4842-3207-1)."],"metadata":{"id":"aDjF76ImzxKN"}},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"jeqbjUnXy2TX"},"outputs":[],"source":["# Import necessary dependencies\n","# We wil use matplotlib and seaborn for exploratory data analysis and visualizations\n","import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","from collections import Counter\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.preprocessing import LabelEncoder\n","\n","%matplotlib inline"]},{"cell_type":"markdown","source":["#Download Wine Quality Datasets"],"metadata":{"id":"saX_8Cah4fxQ"}},{"cell_type":"code","source":["!pip install wget\n","!python -m wget -o winequality-red.csv \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\"\n","!python -m wget -o winequality-white.csv \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-white.csv\"\n","!python -m wget -o winequality.names \"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality.names\""],"metadata":{"id":"Rq_CPonD4mg0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pbb4BZzYy2TY"},"source":["# Load and merge datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5u2b3VtPy2TZ"},"outputs":[],"source":["white_wine = pd.read_csv('winequality-white.csv', sep=';')\n","red_wine = pd.read_csv('winequality-red.csv', sep=';')\n","\n","# store wine type as an attribute\n","red_wine['wine_type'] = 'red'   \n","white_wine['wine_type'] = 'white'\n","# bucket wine quality scores into qualitative quality labels\n","# Wine quality scores of 3, 4, and 5 are mapped to low quality,\n","# 6 and 7 are mapped to medium quality, 8 and 9 are mapped to high quality \n","# wines under the quality_label attribute. \n","red_wine['quality_label'] = red_wine['quality'].apply(lambda value: 'low' \n","                                                          if value <= 5 else 'medium' \n","                                                              if value <= 7 else 'high')\n","red_wine['quality_label'] = pd.Categorical(red_wine['quality_label'], \n","                                           categories=['low', 'medium', 'high'])\n","\n","white_wine['quality_label'] = white_wine['quality'].apply(lambda value: 'low' \n","                                                              if value <= 5 else 'medium' \n","                                                                  if value <= 7 else 'high')\n","white_wine['quality_label'] = pd.Categorical(white_wine['quality_label'], \n","                                             categories=['low', 'medium', 'high'])\n","\n","# merge red and white wine datasets\n","wines = pd.concat([red_wine, white_wine])\n","# re-shuffle records just to randomize data points\n","wines = wines.sample(frac=1, random_state=42).reset_index(drop=True)"]},{"cell_type":"markdown","metadata":{"id":"6NisWsPzy2Ta"},"source":["# Understand dataset features and values"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NNVKwRe6y2Ta"},"outputs":[],"source":["print(white_wine.shape, red_wine.shape)\n","print(wines.info())"]},{"cell_type":"markdown","source":["We have 4898 white wine data points and 1599 red wine data points. The\n","merged dataset contains a total of 6497 data points and we also get an idea of numeric and categorical\n","attributes."],"metadata":{"id":"K3thHV0SEuIf"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"23TI5msyy2Tb"},"outputs":[],"source":["# Let’s take a peek at our dataset to see some sample data points.\n","wines.head()"]},{"cell_type":"markdown","source":["#Utilty functions for model evaluation"],"metadata":{"id":"HDPzEHdm26N7"}},{"cell_type":"code","source":["from sklearn import metrics\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.base import clone\n","from sklearn.preprocessing import label_binarize\n","from scipy import interp\n","from sklearn.metrics import roc_curve, auc \n","\n","def get_metrics(true_labels, predicted_labels):\n","    \n","    print('Accuracy:', np.round(\n","                        metrics.accuracy_score(true_labels, \n","                                               predicted_labels),\n","                        4))\n","    print('Precision:', np.round(\n","                        metrics.precision_score(true_labels, \n","                                               predicted_labels,\n","                                               average='weighted'),\n","                        4))\n","    print('Recall:', np.round(\n","                        metrics.recall_score(true_labels, \n","                                               predicted_labels,\n","                                               average='weighted'),\n","                        4))\n","    print('F1 Score:', np.round(\n","                        metrics.f1_score(true_labels, \n","                                               predicted_labels,\n","                                               average='weighted'),\n","                        4))\n","def display_classification_report(true_labels, predicted_labels, classes=[1,0]):\n","\n","    report = metrics.classification_report(y_true=true_labels, \n","                                           y_pred=predicted_labels, \n","                                           labels=classes) \n","    print(report)\n","    \n","def display_confusion_matrix(true_labels, predicted_labels, classes=[1,0]):\n","    \n","    total_classes = len(classes)\n","    level_labels = [total_classes*[0], list(range(total_classes))]\n","    cm = metrics.confusion_matrix(y_true=true_labels, y_pred=predicted_labels, \n","                                  labels=classes)\n","    cm_frame = pd.DataFrame(data=cm, \n","                            columns=pd.MultiIndex(levels=[['Predicted:'], classes], \n","                                                  codes=level_labels), \n","                            index=pd.MultiIndex(levels=[['Actual:'], classes], \n","                                                codes=level_labels)) \n","    print(cm_frame) \n","\n","def display_model_performance_metrics(true_labels, predicted_labels, classes=[1,0]):\n","    print('Model Performance metrics:')\n","    print('-'*30)\n","    get_metrics(true_labels=true_labels, predicted_labels=predicted_labels)\n","    print('\\nModel Classification report:')\n","    print('-'*30)\n","    display_classification_report(true_labels=true_labels, predicted_labels=predicted_labels, \n","                                  classes=classes)\n","    print('\\nPrediction Confusion Matrix:')\n","    print('-'*30)\n","    display_confusion_matrix(true_labels=true_labels, predicted_labels=predicted_labels, \n","                             classes=classes)\n","\n","def plot_model_roc_curve(clf, features, true_labels, label_encoder=None, class_names=None):\n","    \n","    ## Compute ROC curve and ROC area for each class\n","    fpr = dict()\n","    tpr = dict()\n","    roc_auc = dict()\n","    if hasattr(clf, 'classes_'):\n","        class_labels = clf.classes_\n","    elif label_encoder:\n","        class_labels = label_encoder.classes_\n","    elif class_names:\n","        class_labels = class_names\n","    else:\n","        raise ValueError('Unable to derive prediction classes, please specify class_names!')\n","    n_classes = len(class_labels)\n","    y_test = label_binarize(true_labels, classes=class_labels)\n","    if n_classes == 2:\n","        if hasattr(clf, 'predict_proba'):\n","            prob = clf.predict_proba(features)\n","            y_score = prob[:, prob.shape[1]-1] \n","        elif hasattr(clf, 'decision_function'):\n","            prob = clf.decision_function(features)\n","            y_score = prob[:, prob.shape[1]-1]\n","        else:\n","            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n","        \n","        fpr, tpr, _ = roc_curve(y_test, y_score)      \n","        roc_auc = auc(fpr, tpr)\n","        plt.plot(fpr, tpr, label='ROC curve (area = {0:0.2f})'\n","                                 ''.format(roc_auc),\n","                 linewidth=2.5)\n","        \n","    elif n_classes > 2:\n","        if hasattr(clf, 'predict_proba'):\n","            y_score = clf.predict_proba(features)\n","        elif hasattr(clf, 'decision_function'):\n","            y_score = clf.decision_function(features)\n","        else:\n","            raise AttributeError(\"Estimator doesn't have a probability or confidence scoring system!\")\n","\n","        for i in range(n_classes):\n","            fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n","            roc_auc[i] = auc(fpr[i], tpr[i])\n","\n","        ## Compute micro-average ROC curve and ROC area\n","        fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n","        roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n","\n","        ## Compute macro-average ROC curve and ROC area\n","        # First aggregate all false positive rates\n","        all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n","        # Then interpolate all ROC curves at this points\n","        mean_tpr = np.zeros_like(all_fpr)\n","        for i in range(n_classes):\n","            mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n","        # Finally average it and compute AUC\n","        mean_tpr /= n_classes\n","        fpr[\"macro\"] = all_fpr\n","        tpr[\"macro\"] = mean_tpr\n","        roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n","\n","        ## Plot ROC curves\n","        plt.figure(figsize=(6, 4))\n","        plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n","                 label='micro-average ROC curve (area = {0:0.2f})'\n","                       ''.format(roc_auc[\"micro\"]), linewidth=3)\n","\n","        plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n","                 label='macro-average ROC curve (area = {0:0.2f})'\n","                       ''.format(roc_auc[\"macro\"]), linewidth=3)\n","\n","        for i, label in enumerate(class_labels):\n","            plt.plot(fpr[i], tpr[i], label='ROC curve of class {0} (area = {1:0.2f})'\n","                                           ''.format(label, roc_auc[i]), \n","                     linewidth=2, linestyle=':')\n","    else:\n","        raise ValueError('Number of classes should be atleast 2 or more')\n","        \n","    plt.plot([0, 1], [0, 1], 'k--')\n","    plt.xlim([0.0, 1.0])\n","    plt.ylim([0.0, 1.05])\n","    plt.xlabel('False Positive Rate')\n","    plt.ylabel('True Positive Rate')\n","    plt.title('Receiver Operating Characteristic (ROC) Curve')\n","    plt.legend(loc=\"lower right\")\n","    plt.show()\n","\n","def plot_model_decision_surface(clf, train_features, train_labels,\n","                                plot_step=0.02, cmap=plt.cm.RdYlBu,\n","                                markers=None, alphas=None, colors=None):\n","    \n","    if train_features.shape[1] != 2:\n","        raise ValueError(\"X_train should have exactly 2 columnns!\")\n","    \n","    x_min, x_max = train_features[:, 0].min() - plot_step, train_features[:, 0].max() + plot_step\n","    y_min, y_max = train_features[:, 1].min() - plot_step, train_features[:, 1].max() + plot_step\n","    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n","                         np.arange(y_min, y_max, plot_step))\n","\n","    clf_est = clone(clf)\n","    clf_est.fit(train_features,train_labels)\n","    if hasattr(clf_est, 'predict_proba'):\n","        Z = clf_est.predict_proba(np.c_[xx.ravel(), yy.ravel()])[:,1]\n","    else:\n","        Z = clf_est.predict(np.c_[xx.ravel(), yy.ravel()])    \n","    Z = Z.reshape(xx.shape)\n","    cs = plt.contourf(xx, yy, Z, cmap=cmap)\n","    \n","    le = LabelEncoder()\n","    y_enc = le.fit_transform(train_labels)\n","    n_classes = len(le.classes_)\n","    plot_colors = ''.join(colors) if colors else [None] * n_classes\n","    label_names = le.classes_\n","    markers = markers if markers else [None] * n_classes\n","    alphas = alphas if alphas else [None] * n_classes\n","    for i, color in zip(range(n_classes), plot_colors):\n","        idx = np.where(y_enc == i)\n","        plt.scatter(train_features[idx, 0], train_features[idx, 1], c=color,\n","                    label=label_names[i], cmap=cmap, edgecolors='black', \n","                    marker=markers[i], alpha=alphas[i])\n","    plt.legend()\n","    plt.show()\n"],"metadata":{"id":"mtYsmo4VaLUG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Predicting Wine Quality\n","\n","We will predict the wine quality ratings based on other features. To start with, we\n","will first select our necessary features and separate out the prediction class labels and prepare train and test\n","datasets. We use the prefix **wqp_** in our variables to easily identify them as needed, where **wqp** depicts wine\n","quality prediction."],"metadata":{"id":"EshuYVJGaZKT"}},{"cell_type":"code","source":["wqp_features = wines.iloc[:,:-3]\n","wqp_class_labels = np.array(wines['quality_label'])\n","wqp_label_names = ['low', 'medium', 'high']\n","wqp_feature_names = list(wqp_features.columns)\n","wqp_train_X, wqp_test_X, wqp_train_y, wqp_test_y = train_test_split(wqp_features, wqp_class_labels, \n","                                                                    test_size=0.3, random_state=42)\n","\n","print(Counter(wqp_train_y), Counter(wqp_test_y))\n","print('Features:', wqp_feature_names)"],"metadata":{"id":"aBMtfCKVaxYL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The numbers show us the wine samples for each class and we can also see the feature names which will\n","be used in our feature set."],"metadata":{"id":"P-vSMGE3a4l5"}},{"cell_type":"markdown","source":["##Feature Scaling\n","\n","We will be using a standard scaler in this\n","scenario."],"metadata":{"id":"v3dBYvDYbAj3"}},{"cell_type":"code","source":["# Define the scaler \n","wqp_ss = StandardScaler().fit(wqp_train_X)\n","\n","# Scale the train set\n","wqp_train_SX = wqp_ss.transform(wqp_train_X)\n","\n","# Scale the test set\n","wqp_test_SX = wqp_ss.transform(wqp_test_X)"],"metadata":{"id":"VPL5QwIpbO6k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Train, Predict & Evaluate Model using Decision Tree\n","The main advantage of decision tree based models is model\n","interpretability, since it is quite easy to understand and interpret the decision rules which led to a specific\n","model prediction. Besides this, other advantages include the model’s ability to handle both categorical\n","and numeric data with ease as well as multi-class classification problems. Trees can be even visualized to\n","understand and interpret decision rules better."],"metadata":{"id":"2p3HzmG0blZh"}},{"cell_type":"markdown","source":["###Train the model using DecisionTreeClassifer"],"metadata":{"id":"66IV1ND_0IU0"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeClassifier\n","\n","wqp_dt = DecisionTreeClassifier()\n","wqp_dt.fit(wqp_train_SX, wqp_train_y)\n","\n"],"metadata":{"id":"KMh1giqObxdH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Evaluate model performance"],"metadata":{"id":"4DJq-KA_rjm8"}},{"cell_type":"code","source":["# let’s predict the wine types for our test data samples and evaluate the performance.\n","wqp_dt_predictions = wqp_dt.predict(wqp_test_SX)\n","\n","display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_dt_predictions, \n","                                      classes=wqp_label_names)"],"metadata":{"id":"hacpZPrHb-Ll"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We get an overall F1 Score and model accuracy of approximately 73%. \n","\n","Looking at the class based statistics; we can see the recall for the high quality\n","wine samples is pretty bad since a lot of them have been misclassified into medium and low quality ratings.\n","This is kind of expected since we do not have a lot of training samples for high quality wine if you remember\n","our training sample sizes from earlier. Considering low and high quality rated wine samples, we should at\n","least try to see if we can prevent our model from predicting a low quality wine as high and similarly prevent\n","predicting a high quality wine as low."],"metadata":{"id":"drwr0kBNcVE7"}},{"cell_type":"markdown","source":["###Model Interpretation"],"metadata":{"id":"rNUM5Wj2rKMy"}},{"cell_type":"markdown","source":["####Visualize Feature Importances from Decision Tree Model"],"metadata":{"id":"PVgC2kDWrZOg"}},{"cell_type":"code","source":["wqp_dt_feature_importances = wqp_dt.feature_importances_\n","wqp_dt_feature_names, wqp_dt_feature_scores = zip(*sorted(zip(wqp_feature_names, wqp_dt_feature_importances), \n","                                                          key=lambda x: x[1]))\n","y_position = list(range(len(wqp_dt_feature_names)))\n","plt.barh(y_position, wqp_dt_feature_scores, height=0.6, align='center')\n","plt.yticks(y_position , wqp_dt_feature_names)\n","plt.xlabel('Relative Importance Score')\n","plt.ylabel('Feature')\n","t = plt.title('Feature Importances for Decision Tree')"],"metadata":{"id":"JCzpeRfKqZF0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can clearly observe that the most important features have changed as compared to\n","our previous model. *Alcohol* and *volatile acidity* occupy the top two ranks and *total sulfur dioxide*\n","seems to be one of the most important features for classifying both wine type and quality."],"metadata":{"id":"vYK6tpgW0a8H"}},{"cell_type":"markdown","source":["####Visualize the Decision Tree"],"metadata":{"id":"2NcRRSCb0qL3"}},{"cell_type":"code","source":["from graphviz import Source\n","from sklearn import tree\n","from IPython.display import Image\n","\n","graph = Source(tree.export_graphviz(wqp_dt, out_file=None, class_names=wqp_label_names,\n","                                    filled=True, rounded=True, special_characters=False,\n","                                    feature_names=wqp_feature_names, max_depth=3))\n","png_data = graph.pipe(format='png')\n","with open('dtree_structure.png','wb') as f:\n","    f.write(png_data)\n","\n","Image(png_data)"],"metadata":{"id":"BsVsCzYp0xtp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Our decision tree model has a huge number of nodes and branches hence we visualized our tree for a\n","max depth of three.\n","\n","You can start observing the decision rules from the tree\n","in the figure where the starting split is determined by the rule of alcohol <= -0.128 and with each\n","yes\\no decision branch split, we have further decision nodes as we descend into the tree at each depth level.\n","The class variable is what we are trying to predict, i.e. wine quality being low, medium, or high and value\n","determines the total number of samples at each class present in the current decision node at each instance.\n","\n","The gini parameter is basically the criterion which is used to determine and measure the quality of the split\n","at each decision node. Best splits can be determined by metrics like gini impurity\\gini index or information\n","gain, a metric that helps in minimizing the probability of\n","misclassification. "],"metadata":{"id":"ys3iimL-1cLU"}},{"cell_type":"markdown","source":["##Train, Predict & Evaluate Model using Random Forests\n","\n","In the random\n","forest model, each base learner is a decision tree model trained on a bootstrap sample of the training data.\n","Besides this, when we want to split a decision node in the tree, the split is chosen from a random subset of all\n","the features instead of taking the best split from all the features."],"metadata":{"id":"sJHJDLr4-TO7"}},{"cell_type":"markdown","source":["####Train the model using RandomForestClassifier"],"metadata":{"id":"dTivhiYA-kyo"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","# train the model\n","wqp_rf = # Your code goes here\n","wqp_rf.fit(# Your code goes here)"],"metadata":{"id":"Spz0Y23m-oYy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["####Evaluate model performance"],"metadata":{"id":"ylbSRgYx-pji"}},{"cell_type":"code","source":["# predict and evaluate performance\n","wqp_rf_predictions = # Your code goes here\n","display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_rf_predictions, \n","                                      classes=wqp_label_names)"],"metadata":{"id":"QhOMIdUt-trw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model prediction results on the test dataset depict an overall F1 Score and model accuracy of\n","approximately 80%. This is definitely an improvement of 7% from what we obtained\n","with just decision trees proving that ensemble learning is working better."],"metadata":{"id":"ljc9JIOf-4Kn"}},{"cell_type":"markdown","source":["##Hyperparameter tuning with Grid Search & Cross Validation\n","\n","Another way to further improve on this result is model tuning. To be more specific, models have\n","hyperparameters that can be tuned.\n","\n","Hyperparameters are also known as meta-parameters\n","and are usually set before we start the model training process. These hyperparameters do not have any\n","dependency on being derived from the underlying data on which the model is trained. Usually these\n","hyperparameters represent some high level concepts or knobs, which can be used to tweak and tune the\n","model during training to improve its performance. Our random forest model has several hyperparameters as shown below."],"metadata":{"id":"DCE_rw_y_VsW"}},{"cell_type":"code","source":["print(wqp_rf.get_params())"],"metadata":{"id":"Ru5ahAXH_tAf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Get the best hyperparameter values using Grid Search"],"metadata":{"id":"osg8GySu_0QN"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","param_grid = {\n","                'n_estimators': [100, 200, 300, 500], \n","                'max_features': ['auto', None, 'log2']    \n","              }\n","\n","wqp_clf = GridSearchCV(RandomForestClassifier(random_state=42), param_grid, cv=5,\n","                       scoring='accuracy')\n","wqp_clf.fit(wqp_train_SX, wqp_train_y)\n","print(wqp_clf.best_params_)"],"metadata":{"id":"AMvl3Un-_4jt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We have 500 estimators and auto maximum features which represents the square root of the total\n","number of features to be considered during the best split operations."],"metadata":{"id":"X5lfrusiAEgg"}},{"cell_type":"markdown","source":["###View grid search results"],"metadata":{"id":"yEJ1P5AWAN4t"}},{"cell_type":"code","source":["results = wqp_clf.cv_results_\n","for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n","    print(param, round(score_mean, 4), round(score_sd, 4))"],"metadata":{"id":"wYu1U752AQas"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The output shows the selected hyperparameter combinations and its corresponding mean\n","accuracy and standard deviation values across the grid."],"metadata":{"id":"hsbdKvyOAXQR"}},{"cell_type":"markdown","source":["##Train, Predict & Evaluate Random Forest Model with tuned hyperparameters"],"metadata":{"id":"ILfBHx-yAcwA"}},{"cell_type":"code","source":["wqp_rf = RandomForestClassifier(#Your code goes here)\n","wqp_rf.fit(wqp_train_SX, wqp_train_y)\n","\n","wqp_rf_predictions = wqp_rf.predict(wqp_test_SX)\n","display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_rf_predictions, \n","                                      classes=wqp_label_names)"],"metadata":{"id":"8ijzT_UWAhVx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model prediction results on the test dataset improved the overall F1 Score and model accuracy a little bit from the initial random forest model."],"metadata":{"id":"emMYH-3SBh47"}},{"cell_type":"markdown","source":["##Train, Predict & Evaluate Model using Extreme Gradient Boosting\n","\n","Another way of modeling ensemble based methods is boosting. A very popular method is XGBoost\n","which stands for Extreme Gradient Boosting. It is a variant of the Gradient Boosting Machines (GBM)\n","model. This model is extremely popular in the Data Science community owing to its superior performance\n","in several Data Science challenges and competitions especially on Kaggle."],"metadata":{"id":"MtgSx4sZCB9G"}},{"cell_type":"markdown","source":["###Load and set dependencies"],"metadata":{"id":"OsItIMepCNu8"}},{"cell_type":"code","source":["import os\n","\n","mingw_path = r'C:\\mingw-w64\\mingw64\\bin'\n","os.environ['PATH'] = mingw_path + ';' + os.environ['PATH']\n","\n","import xgboost as xgb"],"metadata":{"id":"w7YdJO_FCTvv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Train the model using XGBClassifier"],"metadata":{"id":"D-eyFLb8CZIT"}},{"cell_type":"code","source":["wqp_xgb_model = #Your code goes here\n","wqp_xgb_model.fit(wqp_train_SX, wqp_train_y)"],"metadata":{"id":"6utA2jvgCe5x"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Predict and Evaluate Model"],"metadata":{"id":"a5oTJWxHCh09"}},{"cell_type":"code","source":["wqp_xgb_predictions = # Your code goes here\n","display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_xgb_predictions, \n","                                      classes=wqp_label_names)"],"metadata":{"id":"SYjnaS_oClHq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Get the best hyperparameter values"],"metadata":{"id":"zGAJy0AsCqjA"}},{"cell_type":"code","source":["param_grid = {\n","                'n_estimators': [100, 200, 300], \n","                'max_depth': [5, 10, 15],\n","                'learning_rate': [0.3, 0.5]\n","              }\n","\n","wqp_clf = GridSearchCV(xgb.XGBClassifier(tree_method='exact', seed=42), param_grid, \n","                       cv=5, scoring='accuracy')\n","wqp_clf.fit(wqp_train_SX, wqp_train_y)\n","print(wqp_clf.best_params_)"],"metadata":{"id":"814q9v4JCuHQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###View grid search results"],"metadata":{"id":"SAAzCbBwDLFa"}},{"cell_type":"code","source":["results = wqp_clf.cv_results_\n","for param, score_mean, score_sd in zip(results['params'], results['mean_test_score'], results['std_test_score']):\n","    print(param, round(score_mean, 4), round(score_sd, 4))"],"metadata":{"id":"yNgkhIXsDOJc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Train, Predict & Evaluate Extreme Gradient Boosted Model with tuned hyperparameters"],"metadata":{"id":"pSFKft-ZDSCy"}},{"cell_type":"code","source":["wqp_xgb_model = xgb.XGBClassifier(seed=42, # Your code goes here)\n","wqp_xgb_model.fit(wqp_train_SX, wqp_train_y)\n","\n","wqp_xgb_predictions = wqp_xgb_model.predict(wqp_test_SX)\n","display_model_performance_metrics(true_labels=wqp_test_y, predicted_labels=wqp_xgb_predictions, \n","                                      classes=wqp_label_names)"],"metadata":{"id":"31s8YhPxDVZN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The model prediction results on the test dataset depict an overall F1 Score and model accuracy of\n","approximately 79%. Though random forests perform slightly better, it definitely\n","performs better than a basic model like a decision tree."],"metadata":{"id":"2WzfyQqwEVaV"}},{"cell_type":"markdown","source":["##Model Interpretation"],"metadata":{"id":"DCmwHc_IEfc3"}},{"cell_type":"markdown","source":["###Comparative analysis of Model Feature importances"],"metadata":{"id":"FrNdI6umEjjb"}},{"cell_type":"code","source":["# Install skater package\n","!pip install git+https://github.com/oracle/Skater.git"],"metadata":{"id":"eeqZySwQEx2S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from skater.core.explanations import Interpretation\n","from skater.model import InMemoryModel\n","# leveraging skater for feature importances\n","interpreter = Interpretation(wqp_test_SX, feature_names=wqp_feature_names)\n","wqp_im_model = InMemoryModel(wqp_rf.predict_proba, examples=wqp_train_SX, target_names=wqp_rf.classes_)\n","# retrieving feature importances from the scikit-learn estimator\n","wqp_rf_feature_importances = wqp_rf.feature_importances_\n","wqp_rf_feature_names, wqp_rf_feature_scores = zip(*sorted(zip(wqp_feature_names, wqp_rf_feature_importances), \n","                                                          key=lambda x: x[1]))\n","# plot the feature importance plots\n","f, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n","t = f.suptitle('Feature Importances for Random Forest', fontsize=12)\n","f.subplots_adjust(top=0.85, wspace=0.6)\n","y_position = list(range(len(wqp_rf_feature_names)))\n","ax1.barh(y_position, wqp_rf_feature_scores, height=0.6, align='center', tick_label=wqp_rf_feature_names)\n","ax1.set_title(\"Scikit-Learn\")\n","ax1.set_xlabel('Relative Importance Score')\n","ax1.set_ylabel('Feature')\n","plots = interpreter.feature_importance.plot_feature_importance(wqp_im_model, ascending=True, ax=ax2)\n","ax2.set_title(\"Skater\")\n","ax2.set_xlabel('Relative Importance Score')\n","ax2.set_ylabel('Feature')"],"metadata":{"id":"GsFop5AvEp4W"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can clearly observe from Figure that the most important features are consistent across the\n","two plots, which is expected considering we are just using different interfaces on the same model. The"],"metadata":{"id":"JLdZrtf9GPVf"}},{"cell_type":"markdown","source":["###Visualize Model ROC Curve"],"metadata":{"id":"FKash42vGVfV"}},{"cell_type":"code","source":["plot_model_roc_curve(wqp_rf, wqp_test_SX, wqp_test_y)"],"metadata":{"id":"-gkINTT_GYB2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The AUC is pretty good based on what we see. The dotted lines indicate the per-class ROC curves and\n","the lines in bold are the macro and micro-average ROC curves."],"metadata":{"id":"KbdPCEIyGiBr"}},{"cell_type":"markdown","source":["###Visualize Model decision surface"],"metadata":{"id":"2yW1UdTVGmo6"}},{"cell_type":"code","source":["feature_indices = [i for i, feature in enumerate(wqp_feature_names) \n","                       if feature in ['alcohol', 'volatile acidity']]\n","plot_model_decision_surface(clf=wqp_rf, train_features=wqp_train_SX[:, feature_indices], \n","                      train_labels=wqp_train_y, plot_step=0.02, cmap=plt.cm.RdYlBu,\n","                      markers=[',', 'd', '+'], alphas=[1.0, 0.8, 0.5], colors=['r', 'b', 'y'])"],"metadata":{"id":"MuiWN-IoGsN5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["###Interpreting Model Predictions"],"metadata":{"id":"dsSRzf_uG-fI"}},{"cell_type":"code","source":["from skater.core.local_interpretation.lime.lime_tabular import LimeTabularExplainer\n","\n","exp = LimeTabularExplainer(wqp_train_SX, feature_names=wqp_feature_names, \n","                           discretize_continuous=True, \n","                           class_names=wqp_rf.classes_)\n","# Model interpretation for our wine quality model's prediction for a low quality wine\n","exp.explain_instance(wqp_test_SX[10], wqp_rf.predict_proba, top_labels=1).show_in_notebook()"],"metadata":{"id":"c-9F8hLRHBvu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The figure above shows us the features that were primarily responsible for the model to\n","predict the wine quality as low. We can see that the most important feature was alcohol, which makes sense\n","considering what we obtained in our analyses so far from feature importances and model decision surface\n","interpretations. The values for each corresponding feature depicted here are the scaled values obtained after\n","feature scaling."],"metadata":{"id":"9aVDodf8H8u8"}},{"cell_type":"code","source":["# Model interpretation for our wine quality model's prediction for a high quality wine\n","# Your code goes here"],"metadata":{"id":"aNV9x4yZHts-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see the features responsible for the model correctly\n","predicting the wine quality as high and the primary feature was again alcohol by volume (besides other\n","features like density, volatile acidity, and so on). Also you can notice a stark difference in the scaled\n","values of alcohol for the two instances"],"metadata":{"id":"jCOINTebIKYU"}},{"cell_type":"markdown","source":["###Visualizing partial dependencies\n","\n","In general, partial dependence helps describe the marginal impact\n","or influence of a feature on the model prediction decision by holding the other features constant. Because it\n","is very difficult to visualize high dimensional feature spaces, typically one or two influential and important\n","features are used to visualize partial dependence plots."],"metadata":{"id":"azSLfkIwIV5o"}},{"cell_type":"markdown","source":["####one-way partial dependence"],"metadata":{"id":"KfgBAUQyI_QZ"}},{"cell_type":"code","source":["# plot one-way partial dependence plots for our model prediction function \n","# based on the most important feature, alcohol.\n","axes_list = interpreter.partial_dependence.plot_partial_dependence(['alcohol'], wqp_im_model, \n","                                                                   grid_resolution=100, \n","                                                                   with_variance=True,\n","                                                                   figsize = (6, 4))\n","axs = axes_list[0][3:]\n","[ax.set_ylim(0, 1) for ax in axs];"],"metadata":{"id":"u4m0r3rQIlPL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that with an increase in the quantity of alcohol content, the\n","confidence\\probability of the model predictor increases in predicting the wine to be either medium or high\n","and similarly it decreases for the probability of wine to be of low quality. This shows there is definitely some\n","relationship between the class predictions with the alcohol content and again the influence of alcohol for\n","predictions of class high is pretty low, which is expected considering training samples for high quality wine\n","are less."],"metadata":{"id":"lFpPaKRzI18d"}},{"cell_type":"markdown","source":["####two-way partial dependence"],"metadata":{"id":"ilP-wnnMJHwB"}},{"cell_type":"code","source":["# plot wwo-way partial dependence plots for our random forest model predictor \n","# based on alcohol and volatile acidity\n","plots_list = # Your code goes here\n","axs = plots_list[0][3:]\n","[ax.set_zlim(0, 1) for ax in axs];"],"metadata":{"id":"vacmNSWvJMvY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see predicting high\n","quality wine, due to the lack of training data, while some dependency is there for high wine quality class\n","prediction with the increase in alcohol and corresponding decrease in volatile acidity is it quite weak, as\n","we can see in the left most plot. There also seems to be a strong dependency on low wine quality class\n","prediction with the corresponding decrease in alcohol and the increase in volatile acidity levels. This\n","is clearly visible in the rightmost plot. The plot in the middle talks about medium wine quality class\n","predictions. We can observe predictions having a strong dependency with corresponding increase in alcohol\n","and with decrease in volatile acidity levels. This should give you a good foundation on leveraging partial\n","dependence plots to dive deeper into model interpretation."],"metadata":{"id":"5Th4Uh-VJiY5"}}],"metadata":{"anaconda-cloud":{},"kernelspec":{"display_name":"Python [conda root]","language":"python","name":"conda-root-py"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"Day_4_Exercise_3_Predicting_Wine_Quality.ipynb","provenance":[{"file_id":"13JWYttF88LiLbb8wi8FUS6RVUgnklc15","timestamp":1647436534392},{"file_id":"1BAwv-ZWceuHdPiMHaH5pIYTodebTOpPD","timestamp":1647425924042},{"file_id":"1BEYzOscw_o15MGFSkP82XY8ULEb0CBuY","timestamp":1647425788020},{"file_id":"https://github.com/dipanjanS/practical-machine-learning-with-python/blob/master/notebooks/Ch09_Analyzing_Wine_Types_and_Quality/Exploratory%20Data%20Analysis.ipynb","timestamp":1647368180157}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}