{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Day_2_Live_Demo_1_Basic_Data_Cleaning.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNnOuvLBpXCDlL4QLG1IO1R"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Basic Data Cleaning**"],"metadata":{"id":"sFcYiVhbweM9"}},{"cell_type":"markdown","source":["In this tutorial, you will learn:\n","\n","* How to identify and remove column variables that only have a single value.\n","* How to identify and consider column variables with very few unique values.\n","* How to identify and remove rows that contain duplicate observations.\n","\n","Adpated from Jason Brownlee. 2020. [Data Preparation for Machine Learning](https://machinelearningmastery.com/data-preparation-for-machine-learning/)."],"metadata":{"id":"T-xjYFrCQhmD"}},{"cell_type":"markdown","source":["#Messy Dataset\n","\n","\n","Breast cancer dataset classifies breast cancer\n","patient as either a recurrence or no recurrence of cancer. There are 286 examples and nine\n","input variables. \n","\n","You can learn more about the dataset here:\n","* Breast Cancer Dataset ([breast-cancer.csv](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.csv))\n","* Breast Cancer Dataset Description ([breast-cancer.names](https://raw.githubusercontent.com/jbrownlee/Datasets/master/breast-cancer.names))\n","\n","\n","The messy dataset was modified from Breast Cancer Dataset.\n"],"metadata":{"id":"giZ-tn6J7ktc"}},{"cell_type":"markdown","source":["###Download messy data file"],"metadata":{"id":"ibIiSW0g-r4n"}},{"cell_type":"code","source":["!wget \"https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\" -O messy_data.csv\n","!head messy_data.csv"],"metadata":{"id":"4MvsLbWB-hSu","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652839327090,"user_tz":240,"elapsed":519,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"01d35d37-e77d-4a92-ea5d-daeb52e9da22"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-18 02:02:06--  https://raw.githubusercontent.com/udel-cbcb/al_ml_workshop/main/data/messy_data.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 25496 (25K) [text/plain]\n","Saving to: ‘messy_data.csv’\n","\n","\rmessy_data.csv        0%[                    ]       0  --.-KB/s               \rmessy_data.csv      100%[===================>]  24.90K  --.-KB/s    in 0.001s  \n","\n","2022-05-18 02:02:06 (22.6 MB/s) - ‘messy_data.csv’ saved [25496/25496]\n","\n","'40-49','premeno','15-19','0-2','yes',4','3','right','left_up','no','recurrence-events'\n","'50-59','ge40','15-19','0-2','no',4','1','right','central','no','no-recurrence-events'\n","'50-59','ge40','35-39','0-2','no',4','2','left','left_low','no','recurrence-events'\n","'40-49','premeno','35-39','0-2','yes',4','3','right','left_low','yes','no-recurrence-events'\n","'40-49','premeno','30-34','3-5','yes',4','2','left','right_up','no','recurrence-events'\n","'50-59','premeno','25-29','3-5','no',4','2','right','left_up','yes','no-recurrence-events'\n","'50-59','ge40','40-44','0-2','no',4','3','left','left_up','no','no-recurrence-events'\n","'40-49','premeno','10-14','0-2','no',4','2','left','left_up','no','no-recurrence-events'\n","'40-49','premeno','0-4','0-2','no',4','2','right','right_low','no','no-recurrence-events'\n","'40-49','ge40','40-44','15-17','yes',4','2','right','left_up','yes','no-recurrence-events'\n"]}]},{"cell_type":"markdown","source":["#Identify Columns That Contain a Single Value\n"],"metadata":{"id":"sRe1fc_I-MQw"}},{"cell_type":"code","source":["# summarize the number of unique values for each column using pandas\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# summarize the number of unique values in each column using nunique()\n","print(\"Shape of messy data: \", df.shape)\n","print(\"Column\\t#Unique values \")\n","print(df.nunique())"],"metadata":{"id":"Fz3fYIjp_ea4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652839516888,"user_tz":240,"elapsed":154,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"f4bf54bf-b6e2-488d-97ba-62fa6f14029a"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Shape of messy data:  (289, 11)\n","Column\t#Unique values \n","0      6\n","1      3\n","2     11\n","3      7\n","4      2\n","5      1\n","6      3\n","7      2\n","8      5\n","9      2\n","10     2\n","dtype: int64\n"]}]},{"cell_type":"markdown","source":["We can see that column index 5 only has a single value and should be removed."],"metadata":{"id":"OpjQlWtH_qcK"}},{"cell_type":"markdown","source":["#Delete columns that contain a single value"],"metadata":{"id":"1bPtTttf_vga"}},{"cell_type":"code","source":["# delete columns with a single unique value\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# get number of unique values for each column\n","counts = df.nunique()\n","# record columns to delete\n","to_del = [i for i,v in enumerate(counts) if v == 1]\n","print(to_del)\n","# drop useless columns\n","df.drop(to_del, axis=1, inplace=True)\n","print(df.shape)"],"metadata":{"id":"738Abx8f_0-I","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652839582895,"user_tz":240,"elapsed":151,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"33ec97f7-0bf0-4148-a42c-728f40e4fc0e"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","[5]\n","(289, 10)\n"]}]},{"cell_type":"markdown","source":["#Identify columns that have very few values"],"metadata":{"id":"i5bW7pFsAI3N"}},{"cell_type":"code","source":["from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# summarize the number of unique values in each column\n","print(\"Column, Count, <1%\")\n","for i, v in enumerate(df.nunique()):\n","  percentage = float(v) / df.shape[0] * 100\n","  if percentage < 1:\n","    print('%d, %d, %.1f%%' % (i, v, percentage))"],"metadata":{"id":"qW3BwXIOFbfH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652839785699,"user_tz":240,"elapsed":162,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"1ccda87e-0949-4e9a-8d35-3956a276a093"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Column, Count, <1%\n","4, 2, 0.7%\n","5, 1, 0.3%\n","7, 2, 0.7%\n","9, 2, 0.7%\n","10, 2, 0.7%\n"]}]},{"cell_type":"markdown","source":["#Drop columns with unique values less than 1 percent of rows"],"metadata":{"id":"vnjw39eIF2kD"}},{"cell_type":"code","source":["# delete columns where number of unique values is less than 1% of the rows\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# get number of unique values for each column\n","counts = df.nunique()\n","# record columns to delete\n","to_del = [i for i,v in enumerate(counts) if (float(v)/df.shape[0] * 100) < 1]\n","print(\"Columns to delete: \", to_del)\n","# drop useless columns\n","df.drop(to_del, axis=1, inplace=True)\n","print(df.shape)"],"metadata":{"id":"KiPY_mBqGGeQ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652839839969,"user_tz":240,"elapsed":191,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"018d5517-1cff-4baf-9d95-89cac42d00d2"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","Columns to delete:  [4, 5, 7, 9, 10]\n","(289, 6)\n"]}]},{"cell_type":"markdown","source":["#Identify rows that contain duplicate data"],"metadata":{"id":"fOPZfL-JPe7U"}},{"cell_type":"code","source":["# locate rows of duplicate data\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","# calculate duplicates\n","dups = df.duplicated()\n","# report if there are any duplicates\n","print(\"Any duplicates? \", dups.any())\n","# list all duplicate rows\n","print(\"Duplicated rows:\")\n","print(df[dups])"],"metadata":{"id":"WFrSFoZRPnpj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652839893745,"user_tz":240,"elapsed":147,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"46b1d448-55f9-4454-836f-ca59051bb5d2"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["Any duplicates?  True\n","Duplicated rows:\n","          0          1        2      3      4   5    6        7           8   \\\n","17   '60-69'     'ge40'  '15-19'  '0-2'   'no'  4'  '2'  'right'   'left_up'   \n","27   '40-49'  'premeno'  '10-14'  '0-2'   'no'  4'  '1'  'right'   'left_up'   \n","44   '30-39'  'premeno'  '15-19'  '0-2'   'no'  4'  '1'   'left'  'left_low'   \n","65   '50-59'     'ge40'  '15-19'  '0-2'   'no'  4'  '1'  'right'   'central'   \n","117  '60-69'     'ge40'  '10-14'  '0-2'   'no'  4'  '1'   'left'   'left_up'   \n","178  '40-49'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'  'right'  'left_low'   \n","190  '50-59'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'   'left'  'right_up'   \n","214  '40-49'  'premeno'  '20-24'  '0-2'   'no'  4'  '2'  'right'   'left_up'   \n","217  '50-59'  'premeno'  '25-29'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","221  '50-59'     'ge40'  '20-24'  '0-2'   'no'  4'  '3'   'left'   'left_up'   \n","237  '30-39'     'lt40'  '15-19'  '0-2'   'no'  4'  '3'  'right'   'left_up'   \n","240  '50-59'     'ge40'  '40-44'  '6-8'  'yes'  4'  '3'   'left'  'left_low'   \n","246  '60-69'     'ge40'  '15-19'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","268  '30-39'  'premeno'  '35-39'  '0-2'   'no'  4'  '3'   'left'  'left_low'   \n","270  '60-69'     'ge40'  '20-24'  '0-2'   'no'  4'  '1'   'left'  'left_low'   \n","287  '40-49'  'premeno'  '20-24'  '0-2'   'no'  4'  '2'  'right'  'right_up'   \n","288  '50-59'     'ge40'  '40-44'  '0-2'   'no'  4'  '2'   'left'  'left_low'   \n","\n","        9                       10  \n","17    'no'  'no-recurrence-events'  \n","27    'no'  'no-recurrence-events'  \n","44    'no'  'no-recurrence-events'  \n","65    'no'  'no-recurrence-events'  \n","117   'no'  'no-recurrence-events'  \n","178   'no'     'recurrence-events'  \n","190   'no'     'recurrence-events'  \n","214   'no'  'no-recurrence-events'  \n","217   'no'  'no-recurrence-events'  \n","221   'no'  'no-recurrence-events'  \n","237   'no'  'no-recurrence-events'  \n","240  'yes'     'recurrence-events'  \n","246   'no'  'no-recurrence-events'  \n","268   'no'     'recurrence-events'  \n","270   'no'  'no-recurrence-events'  \n","287   'no'  'no-recurrence-events'  \n","288   'no'  'no-recurrence-events'  \n"]}]},{"cell_type":"markdown","source":["#Delete rows that contain duplicate data"],"metadata":{"id":"noMvFc6AP-53"}},{"cell_type":"code","source":["# delete rows of duplicate data from the dataset\n","from pandas import read_csv\n","# load the dataset\n","df = read_csv('messy_data.csv', header=None)\n","print(df.shape)\n","# delete duplicate rows\n","df.drop_duplicates(inplace=True)\n","print(df.shape)"],"metadata":{"id":"G4VBQ_dIQEes","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652839905119,"user_tz":240,"elapsed":145,"user":{"displayName":"Chuming Chen","userId":"17839229876463490728"}},"outputId":"577eb0da-6195-4e3a-849f-df8a37e8e7e0"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["(289, 11)\n","(272, 11)\n"]}]}]}